% Preamble
% =============================================================================

% Class of the document.
\documentclass[12pt,a4paper]{article}
% article : short article.
% report  : mid-length report.
% book    : book or thesis redaction.

% Paragraph skip length (default to 0).
\setlength{\parskip}{1ex}

% Packages
% =============================================================================

% Encoding
% -----------------------------------------------------------------------------

% Babel.
\usepackage[french]{babel}
% FontEnc.
\usepackage[T1]{fontenc}
% InputEnc.
\usepackage[utf8]{inputenc}

% Define \escapeus command to escape underscores.
\makeatletter
\DeclareRobustCommand*{\escapeus}[1]{
    \begingroup\@activeus\scantokens{#1\endinput}\endgroup}
\begingroup\lccode`\~=`\_\relax
    \lowercase{\endgroup\def\@activeus{\catcode`\_=\active \let~\_}}
\makeatother

% Text
% -----------------------------------------------------------------------------

% Acronym.
\usepackage{acronym}
% CsQuote.
\usepackage[style=french,french=guillemets]{csquotes}
% Enumerate.
\usepackage{enumerate}
% HyperRef.
\usepackage[hyperfootnotes=false,hidelinks]{hyperref}
% URL.
\usepackage{url}

% Algorithms
% -----------------------------------------------------------------------------

% Algorithm2E.
\usepackage[french,onelanguage,linesnumbered,ruled,vlined,commentsnumbered]{algorithm2e}

% Source code
% -----------------------------------------------------------------------------

% Listings.
\usepackage{listings}
% Minted.
\usepackage{minted}
% Caption.
\usepackage{caption}
\newenvironment{code}{\captionsetup{type=listing}}{}

% Files
% -----------------------------------------------------------------------------

% FancyVRB.
\usepackage{fancyvrb}
% Redefine \VerbatimInput.
\RecustomVerbatimCommand{\VerbatimInput}{VerbatimInput}
{
    fontsize=\footnotesize,
    frame=lines,         % Top and bottom rule only.
    framesep=1.5em,      % Separation between frame and text.
    rulecolor=\color{red!50!green!50!blue!50!},
    labelposition=topline,
    commandchars=\|\(\), % Escape character and argument delimiters for commands within the verbatim.
    commentchar=*        % Comment character.
}

% Figures
% -----------------------------------------------------------------------------

% GraphicX.
\usepackage{graphicx}
% SVG.
\usepackage{svg}
% WrapFig.
\usepackage{wrapfig}

% Charts
% -----------------------------------------------------------------------------

% PGFPLots
\usepackage{pgfplots}
\pgfplotsset{compat=1.16}
\usepgfplotslibrary{units}

% Mathematics
% -----------------------------------------------------------------------------

% AmsFonts.
\usepackage{amsfonts}
% AmsMath.
\usepackage{amsmath}
% AmsText.
\usepackage{amstext}
% AmsThm.
\usepackage{amsthm}
\newtheorem{prr}{Propriété}
\newtheorem{pro}{Proposition}
\newtheorem{thm}{Théorème}
\newtheorem{lem}{Lemme}
% NumPrint.
\usepackage{numprint}

% Physics
% -----------------------------------------------------------------------------

% Physics.
\usepackage{physics}

% Presentation
% -----------------------------------------------------------------------------

% XColor.
\usepackage{xcolor}

% References
% -----------------------------------------------------------------------------

% CleveRef.
\usepackage{cleveref}

% Structure.
% -----------------------------------------------------------------------------

% Geometry.
\usepackage{geometry}
% PDFLScape.
\usepackage{pdflscape}
% MultiCol.
\usepackage{multicol}
% TitleSec.
\usepackage{titlesec}
\newcommand{\sectionbreak}{\clearpage} % Use a page break before new sections.
% VMargin.
\usepackage{vmargin}
% FootMisc.
\usepackage[bottom]{footmisc}

% Symbols
% -----------------------------------------------------------------------------

% SIUnitX.
\usepackage{siunitx}

% Table
% -----------------------------------------------------------------------------

% Array.
\usepackage{array}
% BookTabs.
\usepackage{booktabs}
% CSVSimple.
\usepackage{csvsimple}

% Document
% =============================================================================

\begin{document}

\title{Analyse de performance et optimisation de code}
\author{AYOUB Pierre -- BONNAFOUS Camille -- FLAMANT Océane}

\maketitle

\begin{figure}[b]
    \centering
    \includegraphics[scale=0.3]{figures/isty.jpg}
\end{figure}

\newpage
\begin{abstract}

La simulation numérique est un procédé informatique visant à modéliser un
phénomène par ordinateur, s'agissant le plus souvent d'un phénomène
physique. Cette modélisation prend forme par des systèmes d'équations
décrivant l'état du système physique représenté à chaque instant. De
nombreux domaines scientifiques convergent vers la simulation
informatique, tel que certaines branches de la physique, de l'analyse
et de l'optimisation mathématique, ou encore le calcul haute
performance en informatique. Enfin, la simulation trouve naturellement
de nombreuses applications concernant des sujets variés, tel que la
simulation du climat et des évènements météorologiques, la simulation
d'essais nucléaires, de l'effet d'un médicament sur un corps, ou encore
des astres et de l'univers. Ce rapport s'articulera donc autour de l'analyse et
de l'optimisation d'un code de calcul, cœur des simulations numériques
présentés ci-dessus.

\end{abstract}

\tableofcontents

\section{Introduction}

Le projet que nous vous présentons aujourd'hui consiste à analyser puis, grâce à
nos mesures, optimiser un code de calcul, appelé kernel. Les mesures doivent
s'effectuer à l'aide de l'instruction \textit{x86} \textit{RDTSC}, et des deux
outils d'analyse de performance suivant : \textit{MAQAO} et \textit{LIKWID}.
\textit{RDTSC} nous permet de mesurer le nombre de cycles entre deux instants,
\textit{MAQAO} rends possible l'exécution d'analyses statiques (\textit{CQA}) et
dynamiques (\textit{LPROF}) d'un binaire, présentées par un rapport haut niveau
à l'aide de \textit{ONE-VIEW}, enfin \textit{LIKWID} permet d'obtenir un grand
nombre de métriques très précises concernant, notamment, l'usage de la mémoire.

Afin d'étudier les différents niveaux de la hiérarchie mémoire, chaque membre du
groupe analysera un niveau qu'il se verra assigné. Ci-dessous la liste des
assignations :
\begin{description}

    \item[Pierre] Cache L1 : 
        \begin{itemize}
            \item Intel Core i7-6600U
            \item 32 kiB L1i, 32 kiB L1d
            \item 256 kiB L2
        \end{itemize}
    \item[Océne ] Cache L2 : 
        \begin{itemize}
            \item Cache L1 : 32K
            \item Cache L2 : 256K
            \item Fréquence 2,40GHz
        \end{itemize}
    \item[Camille] RAM : 
        \begin{itemize}
            \item FICHIER PDF ANNEXE
        \end{itemize}
\end{description} 

Le déroulement du projet s'est effectué en plusieurs étapes distinctes :
\begin{description}
    \item[Analyse du code] Cette phase consiste à analyser le programme d'un
        point de vue d'architecture informatique. Il convient d'étudier les
        choix mis en œuvres afin d'implémenter le ou les calculs nécessaires.
    \item[Protocole expérimental] Une fois l'analyse effectuée, nous
        pouvons en déduire le moyen le plus adapté afin de mesurer les
        performances de notre implémentation. Nous allons donc mettre en
        avant les critères théoriques à atteindre dans nos mesures, puis
        nous exposerons la manière dont nous avons mis ceci en pratique.
    \item[Optimisations et mesures] Grâce au protocole mis en place, nous
        pouvons quantifier la performance du programme. De ce fait, nous serons
        en mesure d'expérimenter différentes techniques d'optimisation sur le
        programme et d'en calculer l'accélération.
\end{description}

\section{Analyse du code}

Présentons notre kernel par son prototype, que nous observons sur le
Listing~\ref{lst.baseline.noopt.prot} Nous voyons qu’il y a 3 variables qui sont manipulées :
\begin{itemize}
    \item $n$ correspond à la taille de nos tableaux.
    \item $a$ est un tableau de \textit{float} à deux dimensions dont la taille
        en fonction de $n$ : $4n^2$ Bytes.
    \item $b$ est un tableau de \textit{double} à une dimension dont la taille
        en fonction de $n$ : $8n$ Bytes.
\end{itemize}

\begin{listing}[h]
    \begin{minted}[linenos,numbersep=5pt,frame=lines,framesep=2mm]{C}
void baseline(unsigned n, float a[n][n], double b[n])
    \end{minted}
    \caption{Prototype du kernel non-optimisé}
    \label{lst.baseline.noopt.prot}
\end{listing}

Nous sommes face à un code de calcul très simple en apparence, illustré dans le
Listing~\ref{lst.baseline.noopt} : deux boucles imbriquées, un branchement, un
calcul mêlant multiplication et exponentiel. Mais plusieurs éléments remarquables qui
risquent de poser problème au niveau de la rapidité d'exécution apparaissent
alors : les boucles impliquent qu'il faut prêter attention au sens de parcours
des tableaux, le branchement nous laisse penser qu'il faudrait essayer de le
supprimer, enfin l'exponentiel et la multiplication sont des opérations lourdes.

\begin{listing}[h]
    \begin{minted}[linenos,numbersep=5pt,frame=lines,framesep=2mm]{C}
for (j = 0; j < n; j++) {
    for (i = 0; i < n; i++) {
        if (j == 0)
            b[i] = 1.0;
        b[i] *= exp(a[i][j]);
    }
}
    \end{minted}
    \caption{Kernel non-optimisé}
    \label{lst.baseline.noopt}
\end{listing}

\section{Protocole expérimental}

La mise en place d'un protocole expérimental de mesure est une étape nécessaire
et cruciale dans tout processus d'optimisation de code. D'une part, le but de ce
protocole est de mettre en lumière les points chauds du programme, c'est-à-dire
les parties du code qui ralentissent considérablement l'exécution des calculs :
ces points chauds seront les cibles de nos optimisations. D'autre part, après
chaque tentative d'optimisation, le protocole doit nous permettre de mesurer
l'impact de cette dernière, qu'il soit positif ou négatif, et enfin de le
quantifier.

\subsection{Théorie}

Lors de nos expériences, de nombreux paramètres peuvent être sujets à des
variations aléatoires ou à des erreurs de mesure, ainsi un résultat peut être
biaisé. Afin d'éviter cela, il est impératif d'utiliser une valeur
représentative de nos différentes mesures : une valeur moyenne ou une valeur
médiane. La valeur médiane semble être un meilleur candidat contrairement à la
moyenne, car cette dernière peut-être fortement modifié par une valeur extrême
qui n'as pas lieu d'être. C'est donc la valeur médiane que nous prendrons des
résultats de nos mesures successives issues des méta-répétitions.

Pour que chaque membre de l'équipe puisse travailler sur son niveau de mémoire
cache, il nous faut trouver la taille des données d'entrée à utiliser. Selon le
prototype présenté dans le Listing~\ref{lst.baseline.noopt.prot}, la taille
totale de nos données d'entrées est de $4n^2 + 8n$, avec $n$ la taille entrée
en paramètre du programme.

\subsubsection{Cache L1}

Ci-dessous les paramètres des mesures :
\begin{enumerate}[(1)]
    \item Taille pour tenir dans L1 : $n = 88$.
        En effet, $(n^2 * 4) + (n * 8) = 31680 B$, sachant que la taille du
        cache est de $32 kiB = 2^{15} = 32768 B$, et que si l'on prend $n =
        90$, on obtient $33120 B$, on a bien : $n = 88 < L1 < n = 90$ avec une
        bonne marge de sécurité.
    \item Nombre de répétitions du warm-up : 1000. Ce nombre est suffisant pour
        avoir ensuite des mesures stables dans tous nos tests car les caches
        sont remplis avec nos tableaux, choisi par observation.
    \item Nombre de répétitions des mesures : on choisit un nombre qui nous permet
        d'avoir à peu près 3 secondes par méta-répétition. Ce nombre varie en
        fonction de la taille de notre tableau. Avoir quelques secondes de
        mesures permet d'avoir une faible marge d'erreur de mesures des cycles
        avec un RDTSC. Par exemple, pour une taille de 89, on peut choisir
        1000000.
\end{enumerate}

\subsubsection{Cache L2}
Pour que les deux tableaux entrent entièrement dans le cache L2, il faut 
que la formule respecte les contraintes suivantes : 
\begin{itemize}
    \item La taille totale doit être plus grande que la taille du cache 
    L1 (1). Pour plus sécurité il a été décidé que la taille totale devait 
    être au moins trois fois plus grande que celle du L1.
    \item L2 partage sa mémoire pour stocker à la fois les instructions, 
    les données et ce qui tourne en background, on ne peut donc en utiliser 
    approximativement que 90\% (2). 
\end{itemize}

Ces deux contraintes peuvent être transformées sous forme d'inéquation :
\begin{itemize}
    \item (1) : $3*TL1 < 4n*n + 8n$,
    \item (2) : $4n*n + 8n \le 0,9*\text{TL2}$
\end{itemize}
Après la résolution de ces équations on obtient $n=156$ comme minimum et $n=242$
comme maximum.

\subsubsection{RAM}

FICHIER PDF ANNEXE

\subsubsection{Analyse de sensibilité}

Une fois que l'on connaît la taille des données a fournir en entrée, 
il faut effectuer une analyse de sensibilité pour les autres paramètres. 
\begin{description}
    \item[Nombre de méta-répétition des mesures] Il nous est donné à 31. C'est
        le nombre donné dans la consigne, qui est suffisant pour avoir un nombre
        de mesure significatives.
    \item[Le nombre de warmup] Il doit se situer entre 1 et 1000. Pour le
        déterminer, il doit être le seul paramètre que l'ont fait varier. On
        fait plusieurs exécutions et, avec les valeurs obtenues, on fait une
        courbe pour voir à partir de quelle valeur cela devient stable. Il faut
        aussi vérifier que
        $\frac{\text{médiane}-\text{minimum}}{\text{minimum}}$ est inférieur à
        5\%. 
    \item[Le nombre de répétitions] On le trouve de la même manière que le
        nombre de warmup.
\end{description}

\subsection{Pratique}

Lors de nos premières tentatives pour trouver les paramètres, nous avons remarqué
que ce qui prenait le plus de temps dans notre noyau de calcul était
l'exponentiel. Afin de pouvoir vérifier si nos paramètres sont corrects, nous
avons donc modifié le fichier \textit{kernel.c} pour que ce soit le temps de
récupération des données qui soit le plus grand. Cette version n'est utilisé que
pour tester la véracité des paramètres trouvés dans la section précédente, et
non pas pour évaluer les performances des optimisations ou des compilateurs.

\subsubsection{Cache L1}

On peut observer une nette différence de performance entre un $n = 88$ et un $n
= 90$, qui se traduit par le fait de tenir ou de ne pas tenir en cache
\textit{L1}. On assigne le programme de calcul au cœur n°2 de la machine, sur
son numéro de thread physique, dans le but de limiter les changements de
contexte et de flush de la mémoire cache. Enfin, les tests sont effectués en
rescue mode, sans interface graphique, avec le minimum de tâches tournant en
arrière-plan.

Toutes les mesures effectuées dans le cache L1 sont automatisées par un script
(\textit{bench.sh}). Ce script permet d'aisément exécuter l'ensemble des tests
dans un environnement idéal, ainsi que d'assurer la reproductibilité du protocole
expérimental, critère important d'une méthode scientifique.

\subsubsection{Cache L2}

Pour vérifier le calcul théorique de la taille des données j'ai utilisé
likwid-perftcr afin de voir si les données transitaient bien par le cache L2.
Après avoir compilé avec gcc uniquement j'ai exécuté l'exécutable avec likwid et
voici les résultats obtenus :
\newline

\begin{tabular}{|c|c|}
  \hline
  n & Data Volume (GByte) \\
  \hline
  100 & 4,24 \\
  \hline
  150 & 14,06 \\
  \hline
  220 & 37,9 \\
  \hline
  235 & 34,4 \\
  \hline
\end{tabular}

On observe que ces résultats sont en corrélation avec les résultats théoriques
en dessous de 156 pratiquement aucune donnée ne passe par le cache L2 et 
quand on se rapproche de 242 une partie des données ne semble plus passer 
dans L2 je suppose donc que ces données vont directement dans le cache L3.
Au vu de ces informations, j'ai choisi de prendre 220 comme taille de 
données.

Voici, ci-contre le graphique obtenu pour trouver le bon nombre de warmup. On
peut observer que le nombre de cycles semble se stabiliser au alentour de 100
warmup. Pour plus de sécurité j'ai choisi 150 pour le nombre de warmup.

\begin{figure}[h]
    \includegraphics[scale=0.8]{figures/L2/L2warmup.png}
    \caption{Sans l'exponentiel}
\end{figure}

J'ai ensuite vérifié avec le calcul de l'exponetiel et on obtient bien le 
même résultat. 

\begin{figure}[h]
    \includegraphics[scale=0.8]{figures/L2/L2warmup2.png}
    \caption{Avec l'exponentiel}
\end{figure}

Pour trouver le bon nombre de répétitions j'ai uniquement fait les tests avec
l'exponentiel. Comme vous pouvez le voir, on peut remarquer que l'ensemble est
stable, les variations sont minimes. J'ai choisit comme nombre de répétitions
1200.
\begin{figure}[h]
    \includegraphics[scale=0.8]{figures/L2/L2repet.png}
    \caption{ }
\end{figure}

\subsubsection{RAM}

FICHIER PDF ANNEXE

\section{Optimisations et mesures}

Dans cette section, nous présentons les résultats des mesures des différentes
tentatives d'optimisation du code. La phase 1 correspond, pour résumer, à
identifier les points chauds et tester différentes configurations de compilation.
La phase 2 correspond, notamment, à une optimisation active du code en y apportant
des modifications.

\subsection{Phase 1}

\subsubsection{Cache L1}

Pour cette première phase de test sur un jeu de donnée dans le niveau de cache
L1, nous avons testé 3 compilateurs (\enquote{gcc}, \enquote{icc},
\enquote{clang}) avec différents jeux de flags de compilation. Nous précisons que
l'intégralité de nos résultats sont disponibles sous formes brut dans les
fichiers/répertoires suivants : \enquote{compil.txt},
\enquote{likwid\_\{ref,opt\}}, \enquote{maqao\_\{ref,opt\}}.

En plus des flags qui sont donnés dans la consigne, nous avons également testé
le flag \enquote{-Ofast} qui permet d'activer des optimisations mathématiques qui
ne respecte pas les standards en vigueur. Un programme qui ne requiert pas une
stabilité numérique très précise obtiendra des gains considérable avec cette
option, cependant, cela peut être dangereux de l'activer sans possibilité de
vérifier les résultats des calculs du kernel. Nous avons ici pris le pari de
l'activer.

Nous avons aussi testés d'autres options ciblées :
fonctions inline, optimisations sur les boucles, sur les fonctions
mathématiques ou encore sur les branchements. La liste ci-dessous présente
les flags qui n'auront pas apporté de gain, ou pire, auront provoqué une accélération
négative par rapport à \enquote{-Ofast -march=native} :
\enquote{-faggressive-loop-optimizations}, \enquote{-fbranch-probabilities},
\enquote{-fdelayed-branch}, \enquote{-fexpensive-optimizations},
\enquote{-finline-functions}, \enquote{-floop-block},
\enquote{-floop-interchange}, \enquote{-floop-unroll-and-jam},
\enquote{-funsafe-math-optimizations}. Cependant, le flag
\enquote{-funroll-all-loops}, permettant de forcer l'unrolling des boucles, nous
aura octroyé un léger gain systématique.

Dans la Table~\ref{tab.compil} est présenté la liste des résultats sur les flags
obligatoires et les flags apportant un gain (les flags inutiles ou ralentissant
ne sont pas inclus pour des soucis de visibilité). Nous pouvons ainsi voir que
c'est \enquote{gcc}, couplé à certaines options, qui est le plus rapide face à
\enquote{clang} et \enquote{icc}. Nous notons tout de même l'efficacité
redoutable de la génération de code spécialement pour l'architecture hôte
(\enquote{-march=native}), permettant d'utiliser les instructions x86 les plus
récentes, et des optimisations mathématiques agressives (\enquote{-Ofast}).

\begin{table}[h]
    \centering
    \begin{tabular}{l|l|l}
        \bfseries Compiler & \bfseries Flags & \bfseries Time (s)
        \csvreader{./L1/compil.txt}{}
        {\\\hline\csvcoli&\csvcolii&\csvcoliii}
    \end{tabular}
    \caption{Benchmarks des compilateurs et flags}
    \label{tab.compil}
\end{table}

Nous avons ensuite utiliser les outils \textit{MAQAO} et \textit{LIKWID} pour
expliquer les différences de performances entre deux versions du code. Après nos
tests avec notre script permettant de détecter les flags permettant d'avoir le
meilleur speed-up, nous allons étudier les différences de performances entre la
version de référence \enquote{gcc -O2} et la version la plus rapide,
\enquote{gcc -Ofast -march=native -funroll-all-loops}.

Procédons tout d'abord à une analyse rapide avec \textit{LIKWID}, les résultats
étant présentés dans la Figure~\ref{fig.likwid_noopt}. Nous pouvons expliquer
la différence de performance par les métriques suivantes concernant la mémoire :
on observe que la version optimisé à fait un nombre significativement moins
important que la version de référence d'éviction de données du cache L1 (2.051.211
vs. 16.140.154), ainsi qu'un ratio de miss bien plus faible dans le cache L2
(0.0001 vs. 0.0259).

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.45]{./figures/L1/likwid_noopt.png}
    \caption{A gauche, la version de référence. A droite, la version avec les
    flags d'optimisation.}
    \label{fig.likwid_noopt}
\end{figure}

Enfin, passons à l'étude avec MAQAO. Sur le page \enquote{Global} présenté par
la Figure~\ref{fig.maqao_noopt_global}, nous pouvons déjà avoir une très bonne
idée des différences entre les deux binaires, justifiant d'une telle
accélération ($\text{acc} = \frac{212}{41} = 5.2$). Premièrement, nous observons
que sur le binaire optimisé, nous passons deux fois plus de temps dans la boucle
que dans la version de référence : cela signifie que les fonctions
mathématiques (multiplication, mais surtout l'exponentiel) ont été
considérablement optimisées. Ensuite, nous voyons que la version de référence
présente deux chemins (Flow Complexity) dans la boucle, tandis que la version
optimisée ne présente qu'un chemin d'exécution possible. Nous notons aussi que
l'efficacité d'accès aux données (Array Access Efficiency) a été augmenté de
20\% dans la version optimisée, sûrement par modifications des boucles
imbriquées. Enfin, nous pouvons imaginer une tentative de vectorisation de la
part du compilateur pour la version optimisée.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{./figures/L1/maqao_noopt_ref_global.png}
    \includegraphics[scale=0.5]{./figures/L1/maqao_noopt_opt_global.png}
    \caption{Au-dessus, la version de référence. En dessous, la version avec les
    flags d'optimisation.}
    \label{fig.maqao_noopt_global}
\end{figure}

Pour finir avec l'analyse de \textit{MAQAO}, sur la
Figure~\ref{fig.maqao_noopt_func}, nous pouvons observer ce qu'à concrètement
fait le compilateur. La fonction exponentielle, qui prenait 14\% du temps, à été
remplacé par une version optimisée \enquote{fini}, ne prenant plus que 1.46\% du
temps. Nous pouvons voir que le linkage de la bibliothèque mathématique
(\textit{libm}) a été remplacé par sa version vectorisée (\textit{libmvec}).
Enfin, nous observons que notre unique boucle à bien été déroulée car nous
trouvons l'ajout d'une \textit{tail loop}.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{./figures/L1/maqao_noopt_ref_func.png}
    \includegraphics[scale=0.5]{./figures/L1/maqao_noopt_opt_func.png}
    \caption{Au-dessus, la version de référence. En dessous, la version avec les
    flags d'optimisation.}
    \label{fig.maqao_noopt_func}
\end{figure}

\subsubsection{Cache L2}

\begin{tabular}{|c|c|c|}
  \hline
	Numéro  & Commande & Résultat (médiane)\\
  \hline
  1 & gcc -O2 & 39,93 \\
    \hline
  2 & gcc -O3 & 37,62 \\
    \hline
  3 & gcc -O3 -march=native & 37,79 \\
    \hline
  4 & icc -O2 & 17,79 \\
    \hline
  5 & icc -O3 & 17,77 \\
    \hline
  6 & icc -O3 -xHost & 17,74 \\
  \hline
\end{tabular}

On remarque avec ces résultats que icc rend l'exection plus rapique que 
gcc néamoins entre les différentes options d'optimisation de change pas 
grand chose contrairement à gcc qui entre O2 et O3 gagne quelques secondes.

A l'aide de likwid on peut observer que le volume de data est différent 
entre l'execution avec icc et gcc est différent, il est d'environ 37 GBytes 
avec gcc et 7 GBytes avec icc. Le miss rate est équivalent entre les deux 
compilateur.



\subsubsection{RAM}

FICHIER PDF ANNEXE

\subsection{Phase 2}

TODO 

\section{Conclusion}

TODO

\newpage
\section*{Acronymes}

\begin{acronym}
\end{acronym}

\end{document}
